{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 5: Where to go from here\n",
    "\n",
    "In our fourth tutorial, we used a Sagemaker training job to create a model on the full data set.\n",
    "We learned that SageMaker training jobs allow us to\n",
    "* monitor and log the training progress,\n",
    "* keep track of the trainings we ran so far,\n",
    "* decouple the actual training process from the notebook that prepares and controls our experimental setup.\n",
    "\n",
    "Our final tutorial is about _model performance_ in terms of _prediction quality_.\\\n",
    "To succeed in our competition, we obviously want to obtain a model that maximizes prediction quality.\n",
    "But how do we get there?\n",
    "\n",
    "To answer this question, we will\n",
    "1. sketch how to approach a data science/machine learning problem in an iterative, hypothesis-driven manner,\n",
    "2. show how to obtain insights for optimizing our model and machine learning (ML) pipeline by debugging and interpreting our model's predictions, configuration, and input data.\n",
    "\n",
    "**NOTE: We will NOT need a GPU for this tutorial notebook. Pick a non GPU instance type to save costs.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3           # For interacting with S3\n",
    "import pandas as pd\n",
    "import sys             # Python system library needed to load custom functions\n",
    "\n",
    "# Imports to run Sagemaker training jobs\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.pytorch import PyTorch\n",
    "from sagemaker.session import Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('../src')  # Add the source directory to the PYTHONPATH. This allows to import local functions and modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import DEFAULT_BUCKET, DEFAULT_REGION  # The name of the S3 bucket that contains the training data\n",
    "from detection_util import create_predictions\n",
    "from gdsc_util import download_and_extract_model, set_up_logging, extract_hyperparams, PROJECT_DIR\n",
    "from tutorial_4_training import load_config as load_config_4\n",
    "from tutorial_5_training import load_config as load_config_5\n",
    "from tutorial_5_training_8_epochs import load_config as load_config_5_with_8_epochs\n",
    "from gdsc_util import load_sections_df\n",
    "from PredictionEvaluator import PredictionEvaluator\n",
    "from gdsc_score import get_leaderboard_score\n",
    "\n",
    "set_up_logging()  # Sets up logging to console and .log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterative, Hypothesis-driven Model Development\n",
    "\n",
    "When we start working on a typical data science/ML problem, many aspects of the problem itself, the underlying data, and the required solution are vague.\\\n",
    "Repeated experimentation is key to develop an adequate solution in a systematic manner.\\\n",
    "Therefore, it is best practice to tackle a data science/ML problem in multiple iterations.\n",
    "\n",
    "The [CRISP-DM process model](https://en.wikipedia.org/wiki/Cross-industry_standard_process_for_data_mining) defines the relevant phases of each iteration: \n",
    "\n",
    "<div style=\"text-align: left\">\n",
    "    <img src=\"figures/crisp_dm.png\" alt=\"CRISP-DM\" style=\"width:40%;\" />\n",
    "</div>\n",
    "\n",
    "Applying this iterative, feedback-driven process allows us to\n",
    "- better understand the business problem that we want to solve,\n",
    "- collect, understand, and prepare required data,\n",
    "- identify and engineer relevant features from our data,\n",
    "- develop/select and tune a suitable model of adequate complexity.\n",
    "\n",
    "### The Scientific Method\n",
    "\n",
    "A good way to plan and to carry out these iterations is to apply the [scientific method](https://en.wikipedia.org/wiki/Scientific_method).\\\n",
    "A key concept of this approach are _hypotheses_:\n",
    "- A hypothesis is a testable statement that can either be true or false. It formulates a belief how a specific measure will influence our model's performance.\n",
    "- Hypotheses are tested by conducting an experiment.\n",
    "\n",
    "__Example:__ \"Extending input data for training and inference by different stainings will increase prediction accuracy.\"\n",
    "\n",
    "Before the first iteration, we create a list of initial hypotheses and sort it by priority. Priorities can be assigned by expected information gain or risk, for instance.\\\n",
    "Having created such an initial list, each iteration looks as follows:\n",
    "1. Select the hypothesis of highest priority.\n",
    "2. Conduct an experiment to test the influence of the measure on the model's performance.\n",
    "3. Investigate the outcome and use the gained insights to derive further hypotheses. Adjust priorities if required. \n",
    "\n",
    "Note that this approach not only helps us in prioritizing our investigations, but also forces us to justify our design decisions.\\\n",
    "As a consequence, different aspects of the problem are better understood and complexity is only added if needed.\\\n",
    "The latter characteristic is especially important when having to adapt to changes in the future.\n",
    "\n",
    "### A First List of Hypotheses\n",
    "\n",
    "With regard to our detection problem, we start testing the following hypotheses in the given order:\n",
    "1. Tuning the MMDetection configuration with respect to the input data will increase prediction quality.\n",
    "2. Ignoring the model's detection score deteriorates prediction quality.\n",
    "3. Overfitting impairs our model's generalizability.\n",
    "\n",
    "In the following sections, we will test these hypotheses to improve our model and guide further investigations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing Model Performance and Tuning the Configuration\n",
    "\n",
    "Hypothesis:\n",
    "> Tuning the MMDetection configuration with respect to the input data will increase prediction quality.\n",
    "\n",
    "To test this hypothesis, let us first take a look at the logs of a training job of Tutorial 4.\\\n",
    "To this end, we navigate to a corresponding SageMaker traning job in the AWS management console and open its log output in CloudWatch (see Tutorial 4).\n",
    "\n",
    "The log shows that, after 4 epochs, we get a recall of about 0.7. This means that 30% of all worm sections are not detected by our model. The score we got assigned on the [GDSC website](https://gdsc.ce.capgemini.com/) was around 75.\\\n",
    "We should be able to do much better than this!\n",
    "It seems that our model is currently not able to extract as much information from the data as needed to provide adequate predictions.\n",
    "\n",
    "Let us check again the configuration we used in Tutorial 4:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ContentTooShortError",
     "evalue": "<urlopen error retrieval incomplete: got only 4960 out of 177867103 bytes>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mContentTooShortError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-dce01946b60a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdata_folder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPROJECT_DIR\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m'data'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mcfg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbase_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_config_4\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_folder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Config:\\n{cfg.pretty_text}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/sagemaker-user/gdsc5-tutorials-public/src/tutorial_4_training.py\u001b[0m in \u001b[0;36mload_config\u001b[0;34m(data_folder)\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0mweights_url\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'https://download.openmmlab.com/mmdetection/v2.0/mask_rcnn/mask_rcnn_r50_caffe_fpn_mstrain-poly_3x_coco/mask_rcnn_r50_caffe_fpn_mstrain-poly_3x_coco_bbox_mAP-0.408__segm_mAP-0.37_20200504_163245-42aa3d00.pth'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mweights_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweights_url\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     \u001b[0mweights_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murlretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mweights_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m     \u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_from\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweights_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/urllib/request.py\u001b[0m in \u001b[0;36murlretrieve\u001b[0;34m(url, filename, reporthook, data)\u001b[0m\n\u001b[1;32m    287\u001b[0m         raise ContentTooShortError(\n\u001b[1;32m    288\u001b[0m             \u001b[0;34m\"retrieval incomplete: got only %i out of %i bytes\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 289\u001b[0;31m             % (read, size), result)\n\u001b[0m\u001b[1;32m    290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mContentTooShortError\u001b[0m: <urlopen error retrieval incomplete: got only 4960 out of 177867103 bytes>"
     ]
    }
   ],
   "source": [
    "data_folder = str(PROJECT_DIR / 'data')\n",
    "cfg, base_file = load_config_4(data_folder)\n",
    "\n",
    "print(f'Config:\\n{cfg.pretty_text}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A very careful look at the configuration (or the MMdetection [tutorial](https://mmdetection.readthedocs.io/en/latest/tutorials/config.html)) reveals the interesting parameter ```cfg.model.test_cfg.rcnn.max_per_img```. It limits the maximal number of sections that can be predicted per image.\n",
    "\n",
    "Currently, this value is set to 100:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg.model.test_cfg.rcnn.max_per_img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intuitively, we should ensure that the parameter value is not smaller than the maximum of the number of bounding boxes per image.\\\n",
    "So how many bounding boxes do we have per image? Let us extend our data analysis we started in Tutorial 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "section_df = load_sections_df(f'{PROJECT_DIR}/data/gdsc_train.csv')\n",
    "nodules = section_df.groupby(['file_name','staining','study']).size().sort_values(ascending=False).reset_index()  # Group the data by nodule image\n",
    "nodules[0].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(nodules.head(5))\n",
    "fraction = len(nodules[nodules[0] > 100])/float(len(nodules))\n",
    "print('Fraction >100: {:.1%}'.format(fraction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the histogram, there are quite many nodules with 100-200 sections.\n",
    "The table above shows that we even have 4 nodules with more than 300 sections. So way more than our current limit of 100.\n",
    "In fact, 19% of all nodules in our training set feature more than 100 sections.\n",
    "\n",
    "So it is valid to assume that the current value of ```cfg.model.test_cfg.rcnn.max_per_img``` is too low.\n",
    "To have some buffer for the test data, we will set it to 400.\n",
    "\n",
    "We have prepared this adjusted configuration in the training code for Tutorial 5.\n",
    "So we just load it in the following step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = str(PROJECT_DIR / 'data')\n",
    "cfg, base_file = load_config_5(data_folder)\n",
    "\n",
    "print(f'max_per_img: {cfg.model.test_cfg.rcnn.max_per_img}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We re-train our model to test if the prediction quality of our model increases.\n",
    "\n",
    "Note that we also extended our training script by two prediction steps on train and test data.\n",
    "These predictions are created automatically after the training is completed. They are stored in two CSV files &mdash; one for \"train\" and another one for \"test\". After downloading and extracting the generated model, both files are located in your project's data folder. This is the same location where you found the generated predictions of Tutorial 4.\n",
    "\n",
    "This functionality will become handy in the course of this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entry_point = 'tutorial_5_training.py'\n",
    "exp_name = entry_point.split('.')[0].replace('_', '-')  # AWS does not allow . and _ as experiment names\n",
    "\n",
    "account_id = boto3.client('sts').get_caller_identity().get('Account')\n",
    "role = get_execution_role()\n",
    "\n",
    "sm_client = boto3.client(\"sagemaker\", region_name=DEFAULT_REGION)\n",
    "sess = Session(sagemaker_client=sm_client)\n",
    "s3_output_location = f\"s3://{sess.default_bucket()}/{exp_name}\"\n",
    "input_channels = {\"train\": f\"s3://{DEFAULT_BUCKET}\"}\n",
    "\n",
    "hyperparameters = extract_hyperparams(entry_point) # custom function to parse the training script and extract config\n",
    "hyperparameters['base_file'] = base_file\n",
    "\n",
    "metrics = [\n",
    "    {\"Name\": \"train:loss_rpn_cls\", \"Regex\": \"loss_rpn_cls: ([0-9\\.]+)\"},\n",
    "    {\"Name\": \"train:loss_rpn_bbox\", \"Regex\": \"loss_rpn_bbox: ([0-9\\.]+)\"},\n",
    "    {\"Name\": \"train:loss_cls\", \"Regex\": \"loss_cls: ([0-9\\.]+)\"},\n",
    "    {\"Name\": \"train:loss_bbox\", \"Regex\": \"loss_bbox: ([0-9\\.]+)\"},\n",
    "    {\"Name\": \"train:loss\", \"Regex\": \"loss: ([0-9\\.]+)\"},\n",
    "    {\"Name\": \"train:accuracy\", \"Regex\": \"acc: ([0-9\\.]+)\"},\n",
    "    {\"Name\": \"train:epoch\", \"Regex\": \"Epoch (\\[[0-9\\.]+\\])\"},\n",
    "    {\"Name\": \"val:epoch\", \"Regex\": \"Epoch\\(val\\) (\\[[0-9]+\\])\"},\n",
    "    {\"Name\": \"val:mAP\", \"Regex\": \"mAP: ([0-9\\.]+)\"},\n",
    "]\n",
    "\n",
    "estimator = PyTorch(\n",
    "    entry_point=entry_point,             # This function will be called by the training job\n",
    "    source_dir=\"../src\",                 # All code in this folder will be copied over\n",
    "    image_uri=f\"954362353459.dkr.ecr.{DEFAULT_REGION}.amazonaws.com/sm-training-custom:torch-1.8.1-cu111-noGPL\",\n",
    "    role=role,\n",
    "    output_path=s3_output_location,\n",
    "    container_log_level=20,             # 10=debug, 20=info\n",
    "    base_job_name=exp_name,\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.g4dn.xlarge\",     # a GPU instance\n",
    "    volume_size=45,\n",
    "    metric_definitions=metrics,\n",
    "    hyperparameters=hyperparameters,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.fit(\n",
    "    input_channels,\n",
    "    wait=False,           # Whether or not the notebook should wait for the job to finish. By setting it to False we can continue working while the job runs on another machine.\n",
    ")\n",
    "\n",
    "# save the name of the experiment to the filesystem so that we can use it later\n",
    "experiment_name = estimator._hyperparameters[\"sagemaker_job_name\"]\n",
    "\n",
    "with open(f'{PROJECT_DIR}/experiment_tut5_max_per_img.txt', 'w+') as f:\n",
    "    f.write(experiment_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-06-07 17:34:09,897 - gdsc_util - INFO - File tutorial-5-training/tutorial-5-training-2022-05-29-10-30-56-102/output/model.tar.gz already exists. Skipping download\n"
     ]
    }
   ],
   "source": [
    "# read the experiment name from the filesystem\n",
    "with open(f'{PROJECT_DIR}/experiment_tut5_max_per_img.txt', 'r') as f:\n",
    "    experiment_name = f.read()\n",
    "    \n",
    "\n",
    "model_location = f'{s3_output_location}/{experiment_name}/output/model.tar.gz'\n",
    "local_model_dir = download_and_extract_model(model_uri=model_location, local_dir='data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the parameter adjustment, you should now get a recall of about 0.8 (note that the previous recall was ~0.7).\\\n",
    "Submitting the results on the [GDSC website](https://gdsc.ce.capgemini.com/) reveals a more disappointing result:\n",
    "_Our score decreased from 75 to 66!_\n",
    "\n",
    "To understand this behavior, recap that the assigned score is the sum of multiple accuracy values our model obtains for different IoU thresholds (see Tutorial 3, for instance).\n",
    "Interestingly, a major difference between the accuracy and the recall metrics is that recall does not take account of _false positives_. \n",
    "\n",
    "So it is valid to assume that the drop in overall performance is due to an increased number of false positives.\n",
    "The following observations supprt this assumption:\n",
    "1. The statistics of the section counts shows that 75% of our nodules do not have more than 86 sections. This is far less than the 400 regions the model can detect. \n",
    "2. Checking the training logs reveals that the model detects ~105k regions with the new configuration, compared to ~95k with the old configuration. Many of these additional regions could be false positives given that most nodules exhibit relatively few sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>994.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>66.083501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>54.383185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>28.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>53.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>86.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>381.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                0\n",
       "count  994.000000\n",
       "mean    66.083501\n",
       "std     54.383185\n",
       "min      1.000000\n",
       "25%     28.250000\n",
       "50%     53.000000\n",
       "75%     86.000000\n",
       "max    381.000000"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nodules.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, our hypothesis did not turn out to be _true_.\n",
    "We still keep the updated configuration since the experiment's outcome supports our idea of investigating the model's detection score in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Avoiding False Detections/Positives\n",
    "\n",
    "Refined hypothesis:\n",
    "> Ignoring the model's detection score deteriorates prediction quality due to false positives.\n",
    "\n",
    "Currently, we consider every bounding box that our model draws to be a detected section &mdash; no matter how _certain_ the model is about it.\\\n",
    "In fact, our model also outputs a detection score for every bounding box. It denotes the model's confidence that it actually found a section.\n",
    "So far, we did not make use of this information.\n",
    "\n",
    "In the following experiment, we will test if we can improve our model's prediction quality by only submitting sections having high detection scores.\n",
    "\n",
    "To recap how the predictions look like, we load the predictions on training data created by the model trained using the updated configuration in the previous section:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>section_id</th>\n",
       "      <th>file_name</th>\n",
       "      <th>xmin</th>\n",
       "      <th>ymin</th>\n",
       "      <th>xmax</th>\n",
       "      <th>ymax</th>\n",
       "      <th>detection_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>102_A.jpg@3297-4216-2777-3165</td>\n",
       "      <td>102_A.jpg</td>\n",
       "      <td>3297</td>\n",
       "      <td>2777</td>\n",
       "      <td>4216</td>\n",
       "      <td>3165</td>\n",
       "      <td>0.978597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>102_A.jpg@3221-3544-5465-6349</td>\n",
       "      <td>102_A.jpg</td>\n",
       "      <td>3221</td>\n",
       "      <td>5465</td>\n",
       "      <td>3544</td>\n",
       "      <td>6349</td>\n",
       "      <td>0.967986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>102_A.jpg@2719-3498-4852-5194</td>\n",
       "      <td>102_A.jpg</td>\n",
       "      <td>2719</td>\n",
       "      <td>4852</td>\n",
       "      <td>3498</td>\n",
       "      <td>5194</td>\n",
       "      <td>0.961160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>102_A.jpg@3528-3867-4057-5085</td>\n",
       "      <td>102_A.jpg</td>\n",
       "      <td>3528</td>\n",
       "      <td>4057</td>\n",
       "      <td>3867</td>\n",
       "      <td>5085</td>\n",
       "      <td>0.954487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>102_A.jpg@2171-2839-1868-2386</td>\n",
       "      <td>102_A.jpg</td>\n",
       "      <td>2171</td>\n",
       "      <td>1868</td>\n",
       "      <td>2839</td>\n",
       "      <td>2386</td>\n",
       "      <td>0.941419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107831</th>\n",
       "      <td>9_D.jpg@3111-3692-2444-3082</td>\n",
       "      <td>9_D.jpg</td>\n",
       "      <td>3111</td>\n",
       "      <td>2444</td>\n",
       "      <td>3692</td>\n",
       "      <td>3082</td>\n",
       "      <td>0.051561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107832</th>\n",
       "      <td>9_D.jpg@2825-3153-1301-1698</td>\n",
       "      <td>9_D.jpg</td>\n",
       "      <td>2825</td>\n",
       "      <td>1301</td>\n",
       "      <td>3153</td>\n",
       "      <td>1698</td>\n",
       "      <td>0.051457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107833</th>\n",
       "      <td>9_D.jpg@2110-2584-1101-1989</td>\n",
       "      <td>9_D.jpg</td>\n",
       "      <td>2110</td>\n",
       "      <td>1101</td>\n",
       "      <td>2584</td>\n",
       "      <td>1989</td>\n",
       "      <td>0.051006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107834</th>\n",
       "      <td>9_D.jpg@903-1636-3752-4494</td>\n",
       "      <td>9_D.jpg</td>\n",
       "      <td>903</td>\n",
       "      <td>3752</td>\n",
       "      <td>1636</td>\n",
       "      <td>4494</td>\n",
       "      <td>0.050689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107835</th>\n",
       "      <td>9_D.jpg@1640-2051-2421-3383</td>\n",
       "      <td>9_D.jpg</td>\n",
       "      <td>1640</td>\n",
       "      <td>2421</td>\n",
       "      <td>2051</td>\n",
       "      <td>3383</td>\n",
       "      <td>0.050467</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>107836 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           section_id  file_name  xmin  ymin  xmax  ymax  \\\n",
       "0       102_A.jpg@3297-4216-2777-3165  102_A.jpg  3297  2777  4216  3165   \n",
       "1       102_A.jpg@3221-3544-5465-6349  102_A.jpg  3221  5465  3544  6349   \n",
       "2       102_A.jpg@2719-3498-4852-5194  102_A.jpg  2719  4852  3498  5194   \n",
       "3       102_A.jpg@3528-3867-4057-5085  102_A.jpg  3528  4057  3867  5085   \n",
       "4       102_A.jpg@2171-2839-1868-2386  102_A.jpg  2171  1868  2839  2386   \n",
       "...                               ...        ...   ...   ...   ...   ...   \n",
       "107831    9_D.jpg@3111-3692-2444-3082    9_D.jpg  3111  2444  3692  3082   \n",
       "107832    9_D.jpg@2825-3153-1301-1698    9_D.jpg  2825  1301  3153  1698   \n",
       "107833    9_D.jpg@2110-2584-1101-1989    9_D.jpg  2110  1101  2584  1989   \n",
       "107834     9_D.jpg@903-1636-3752-4494    9_D.jpg   903  3752  1636  4494   \n",
       "107835    9_D.jpg@1640-2051-2421-3383    9_D.jpg  1640  2421  2051  3383   \n",
       "\n",
       "        detection_score  \n",
       "0              0.978597  \n",
       "1              0.967986  \n",
       "2              0.961160  \n",
       "3              0.954487  \n",
       "4              0.941419  \n",
       "...                 ...  \n",
       "107831         0.051561  \n",
       "107832         0.051457  \n",
       "107833         0.051006  \n",
       "107834         0.050689  \n",
       "107835         0.050467  \n",
       "\n",
       "[107836 rows x 7 columns]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_df = pd.read_csv(f'{data_folder}/{experiment_name}/results_tutorial5_train_epoch_4.csv', sep=';')\n",
    "prediction_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The statistics show us that, in fact, the model has quite low confidence values (<= 0.35) for 50% of the detections:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>xmin</th>\n",
       "      <th>ymin</th>\n",
       "      <th>xmax</th>\n",
       "      <th>ymax</th>\n",
       "      <th>detection_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>107836.000000</td>\n",
       "      <td>107836.000000</td>\n",
       "      <td>107836.000000</td>\n",
       "      <td>107836.000000</td>\n",
       "      <td>107836.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3007.071822</td>\n",
       "      <td>3575.800271</td>\n",
       "      <td>3449.560601</td>\n",
       "      <td>4103.073899</td>\n",
       "      <td>0.426875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1443.705168</td>\n",
       "      <td>1609.879749</td>\n",
       "      <td>1475.469295</td>\n",
       "      <td>1637.756149</td>\n",
       "      <td>0.327494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>243.000000</td>\n",
       "      <td>248.000000</td>\n",
       "      <td>0.050003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1888.000000</td>\n",
       "      <td>2285.000000</td>\n",
       "      <td>2310.000000</td>\n",
       "      <td>2801.000000</td>\n",
       "      <td>0.100776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2837.500000</td>\n",
       "      <td>3474.000000</td>\n",
       "      <td>3295.000000</td>\n",
       "      <td>4042.000000</td>\n",
       "      <td>0.344173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>3953.000000</td>\n",
       "      <td>4792.000000</td>\n",
       "      <td>4420.000000</td>\n",
       "      <td>5373.000000</td>\n",
       "      <td>0.765656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>7862.000000</td>\n",
       "      <td>7965.000000</td>\n",
       "      <td>8141.000000</td>\n",
       "      <td>8191.000000</td>\n",
       "      <td>0.993109</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                xmin           ymin           xmax           ymax  \\\n",
       "count  107836.000000  107836.000000  107836.000000  107836.000000   \n",
       "mean     3007.071822    3575.800271    3449.560601    4103.073899   \n",
       "std      1443.705168    1609.879749    1475.469295    1637.756149   \n",
       "min         0.000000       0.000000     243.000000     248.000000   \n",
       "25%      1888.000000    2285.000000    2310.000000    2801.000000   \n",
       "50%      2837.500000    3474.000000    3295.000000    4042.000000   \n",
       "75%      3953.000000    4792.000000    4420.000000    5373.000000   \n",
       "max      7862.000000    7965.000000    8141.000000    8191.000000   \n",
       "\n",
       "       detection_score  \n",
       "count    107836.000000  \n",
       "mean          0.426875  \n",
       "std           0.327494  \n",
       "min           0.050003  \n",
       "25%           0.100776  \n",
       "50%           0.344173  \n",
       "75%           0.765656  \n",
       "max           0.993109  "
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To investigate how this impacts prediction quality, we first run the evaluation function we use to calculate leaderboard scores against these predictions for different IoU thresholds.\\\n",
    "The reported score is the sum of the _detection_acc@X_ values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-06-07 17:34:34,208 - gdsc_score - INFO - Computing results for threshold: 0.5\n",
      "2022-06-07 17:34:34,209 - PredictionEvaluator - INFO - Matching sections\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 994/994 [01:24<00:00, 11.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-06-07 17:35:58,707 - PredictionEvaluator - INFO - Merging matched sections\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-06-07 17:35:58,850 - PredictionEvaluator - INFO - Done matching sections\n",
      "2022-06-07 17:35:58,854 - PredictionEvaluator - INFO - Evaluating predictions\n",
      "2022-06-07 17:35:58,855 - PredictionEvaluator - INFO - Computing overall scores\n",
      "2022-06-07 17:35:58,873 - gdsc_score - INFO - Computing results for threshold: 0.6\n",
      "2022-06-07 17:35:58,874 - PredictionEvaluator - INFO - Matching sections\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 994/994 [01:25<00:00, 11.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-06-07 17:37:24,070 - PredictionEvaluator - INFO - Merging matched sections\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-06-07 17:37:24,222 - PredictionEvaluator - INFO - Done matching sections\n",
      "2022-06-07 17:37:24,230 - PredictionEvaluator - INFO - Evaluating predictions\n",
      "2022-06-07 17:37:24,234 - PredictionEvaluator - INFO - Computing overall scores\n",
      "2022-06-07 17:37:24,255 - gdsc_score - INFO - Computing results for threshold: 0.7\n",
      "2022-06-07 17:37:24,256 - PredictionEvaluator - INFO - Matching sections\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 994/994 [01:25<00:00, 11.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-06-07 17:38:49,932 - PredictionEvaluator - INFO - Merging matched sections\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-06-07 17:38:50,073 - PredictionEvaluator - INFO - Done matching sections\n",
      "2022-06-07 17:38:50,081 - PredictionEvaluator - INFO - Evaluating predictions\n",
      "2022-06-07 17:38:50,082 - PredictionEvaluator - INFO - Computing overall scores\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'detection_acc@iou0.5': 45.3,\n",
       " 'detection_tp@iou0.5': 54153,\n",
       " 'detection_fp@iou0.5': 53855,\n",
       " 'detection_fn@iou0.5': 11534,\n",
       " 'detection_acc@iou0.6': 41.85,\n",
       " 'detection_tp@iou0.6': 51219,\n",
       " 'detection_fp@iou0.6': 56703,\n",
       " 'detection_fn@iou0.6': 14468,\n",
       " 'detection_acc@iou0.7': 34.89,\n",
       " 'detection_tp@iou0.7': 44896,\n",
       " 'detection_fp@iou0.7': 62977,\n",
       " 'detection_fn@iou0.7': 20791,\n",
       " 'score': 122.04}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ground_truth = load_sections_df(f'{data_folder}/gdsc_train.csv')\n",
    "\n",
    "evaluator = PredictionEvaluator(ground_truth)\n",
    "thresholds = [0.5, 0.6, 0.7]\n",
    "get_leaderboard_score(prediction_df, thresholds, evaluator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we run the function again &mdash; but this time only on detected regions with a confidence score greater than 0.5:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-06-07 17:38:50,126 - gdsc_score - INFO - Computing results for threshold: 0.5\n",
      "2022-06-07 17:38:50,127 - PredictionEvaluator - INFO - Matching sections\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 994/994 [01:14<00:00, 13.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-06-07 17:40:04,391 - PredictionEvaluator - INFO - Merging matched sections\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-06-07 17:40:04,515 - PredictionEvaluator - INFO - Done matching sections\n",
      "2022-06-07 17:40:04,518 - PredictionEvaluator - INFO - Evaluating predictions\n",
      "2022-06-07 17:40:04,519 - PredictionEvaluator - INFO - Computing overall scores\n",
      "2022-06-07 17:40:04,534 - gdsc_score - INFO - Computing results for threshold: 0.6\n",
      "2022-06-07 17:40:04,536 - PredictionEvaluator - INFO - Matching sections\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 994/994 [01:14<00:00, 13.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-06-07 17:41:18,720 - PredictionEvaluator - INFO - Merging matched sections\n",
      "2022-06-07 17:41:18,833 - PredictionEvaluator - INFO - Done matching sections\n",
      "2022-06-07 17:41:18,840 - PredictionEvaluator - INFO - Evaluating predictions\n",
      "2022-06-07 17:41:18,841 - PredictionEvaluator - INFO - Computing overall scores\n",
      "2022-06-07 17:41:18,853 - gdsc_score - INFO - Computing results for threshold: 0.7\n",
      "2022-06-07 17:41:18,854 - PredictionEvaluator - INFO - Matching sections\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 994/994 [01:13<00:00, 13.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-06-07 17:42:32,870 - PredictionEvaluator - INFO - Merging matched sections\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-06-07 17:42:32,991 - PredictionEvaluator - INFO - Done matching sections\n",
      "2022-06-07 17:42:32,998 - PredictionEvaluator - INFO - Evaluating predictions\n",
      "2022-06-07 17:42:33,003 - PredictionEvaluator - INFO - Computing overall scores\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'detection_acc@iou0.5': 58.97,\n",
       " 'detection_tp@iou0.5': 41128,\n",
       " 'detection_fp@iou0.5': 4055,\n",
       " 'detection_fn@iou0.5': 24559,\n",
       " 'detection_acc@iou0.6': 56.47,\n",
       " 'detection_tp@iou0.6': 39982,\n",
       " 'detection_fp@iou0.6': 5120,\n",
       " 'detection_fn@iou0.6': 25705,\n",
       " 'detection_acc@iou0.7': 50.0,\n",
       " 'detection_tp@iou0.7': 36920,\n",
       " 'detection_fp@iou0.7': 8154,\n",
       " 'detection_fn@iou0.7': 28767,\n",
       " 'score': 165.44}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_leaderboard_score(prediction_df[prediction_df.detection_score>0.5], thresholds, evaluator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that, when relying on a minimal detection score, the number of false positives decreases by about an order of magnitude for all IoU thresholds.\\\n",
    "Consequently, we achieve a significantly higher accuracy.\n",
    "Also, precision improves since the ratio of true positives to false positives increased.\n",
    "\n",
    "On the other hand, the raising number of false negatives reflects that our model far more often fails to detect true sections: It doubles for IoU thresholds 0.5 and 0.6. It would be worthwhile to examine the detection score for these sections to learn how certain our model is about them.\n",
    "\n",
    "Next, we test the influence on the leaderboard score by exporting and submitting filtered test predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_df = pd.read_csv(f'{data_folder}/{experiment_name}/results_tutorial5_test_epoch_4.csv', sep=';')\n",
    "restricted_prediction_df = prediction_df[prediction_df.detection_score>0.5]\n",
    "restricted_prediction_df.to_csv(f'{data_folder}/results_tutorial5_epoch_4_min_detection_score.csv', sep=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great news, our assumption was correct! With this measure, we can boost our model's score to 90.\n",
    "\n",
    "**Exercises:**\n",
    "- Pick some nodules from the training data and check which detected sections are below the detection score threshold. Do you agree to sort out these detections?\n",
    "- Use the model of Tutorial 4 together with a detection score threshold of 0.5 to generate predictions on training data. How does the calculated leaderboard score compare to the results we obtained here with ```cfg.model.test_cfg.rcnn.max_per_img = 400```?\n",
    "- Find a threshold that further improves your model's performance. How can you find an adequate threshold with a low number of training runs? A good method saves time, energy, and money. Do not use a brute-force search!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do we Overfit?\n",
    "\n",
    "Having conducted some experiments with the setup of Tutorial 4, we see that the mAP metric stagnates and sometimes starts fluctuating around ~0.65 after 2-3 epochs:\n",
    "<br>\n",
    "<br>\n",
    "<img src=\"figures/overfitting_logs.png\" alt=\"Do we overfit?\" style=\"width:40%;\" />\n",
    "    \n",
    "In addition, we observe that we obtain much higher leaderboard scores on training than on test data. For instance, 165 vs. 90 for the experiment in the previous section.  \n",
    "\n",
    "Hypothesis:\n",
    "> Overfitting impairs our model's generalizability.\n",
    "\n",
    "To test this hypothesis, it is important to understand what _overfitting_ is and how it relates to _underfitting_.\n",
    "\n",
    "### Overfitting\n",
    "\n",
    "_Overfitting_ describes a situation in which a model is able to remember too many little details of the training data.\n",
    "In the worst case, the model simply memorizes the input-output relation for every training example.\\\n",
    "While an overfitting model performs very well on training data, it underperforms on validation/test data.\n",
    "In other words, it lacks ability to _generalize_.\n",
    "\n",
    "Overfitting can be identified by comparing training to validation/test losses:\n",
    "- Overfitting is indicated if the validation/test loss stagnates while the training loss continuous to improve.\n",
    "- If, in further epochs, the validation/test loss starts getting worse again, the model clearly overfits.\n",
    "\n",
    "<div style=\"text-align: left\">\n",
    "    <img src=\"figures/overfitting_loss.png\" alt=\"Overfitting - Loss\" style=\"width:550px;\" />\n",
    "</div>\n",
    "\n",
    "(Figure taken from [IBM Cloud Learn Hub: Overfitting](https://www.ibm.com/cloud/learn/overfitting))\n",
    "\n",
    "### Underfitting\n",
    "\n",
    "_Underfitting_ occurs if the model is not able to capture relevant patterns from the training data.\n",
    "As a result, the model makes a lot of wrong predictions so that its training and validation/test losses remain at a high level.\\\n",
    "There are several reasons for underfitting. For example:\n",
    "- Lack of model complexity\n",
    "- Use of unsuitable or insufficient features\n",
    "- Insufficient number of training iterations\n",
    "\n",
    "### Overfitting vs. Underfitting\n",
    "\n",
    "The following figure compares an underfitting (left), a well generalizing (middle), and an overfitting model (right):\n",
    "\n",
    "<div style=\"text-align: left\">\n",
    "    <img src=\"figures/underfitting_overfitting.png\" alt=\"Overfitting\" style=\"width:75%\" />\n",
    "</div>\n",
    "\n",
    "(Figure taken from [scikit-learn](https://scikit-learn.org/stable/auto_examples/model_selection/plot_underfitting_overfitting.html))\n",
    "\n",
    "\n",
    "**Further reading:**\n",
    "- [This article](https://towardsdatascience.com/overfitting-vs-underfitting-a-complete-example-d05dd7e19765) outlines over-/underfitting in more detail.\n",
    "- [This article](https://towardsdatascience.com/8-simple-techniques-to-prevent-overfitting-4d443da2ef7d) presents different techniques you can apply to deal with overfitting. \n",
    "- [This article](http://scott.fortmann-roe.com/docs/BiasVariance.html) provides a detailed discussion on the [bias-variance tradeoff](https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff) and its relation to over- and underfitting.\n",
    "\n",
    "\n",
    "### Do we Really Overfit?\n",
    "\n",
    "Based on the definitions above, we cannot come to the conclusion that our model overfits.\\\n",
    "The main reason for this is that our training logs do not contain information on how the validation/test loss develops in comparison to the training loss. \n",
    "\n",
    "To check if we overfit, we train a new model with our most recent configuration for 8 epochs. Afterwards, we compute the leaderboard score and compare it with the one we obtained for 4 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-06-07 17:28:07,449 - sagemaker.image_uris - INFO - Defaulting to the only supported framework/algorithm version: latest.\n",
      "2022-06-07 17:28:07,470 - sagemaker.image_uris - INFO - Ignoring unnecessary instance type: None.\n",
      "2022-06-07 17:28:07,914 - sagemaker - INFO - Creating training-job with name: tutorial-5-training-8-epochs-2022-06-07-17-28-07-449\n"
     ]
    }
   ],
   "source": [
    "entry_point = 'tutorial_5_training_8_epochs.py'\n",
    "exp_name = entry_point.split('.')[0].replace('_', '-')  # AWS does not allow . and _ as experiment names\n",
    "\n",
    "account_id = boto3.client('sts').get_caller_identity().get('Account')\n",
    "role = get_execution_role()\n",
    "\n",
    "sm_client = boto3.client(\"sagemaker\", region_name=DEFAULT_REGION)\n",
    "sess = Session(sagemaker_client=sm_client)\n",
    "s3_output_location = f\"s3://{sess.default_bucket()}/{exp_name}\"\n",
    "input_channels = {\"train\": f\"s3://{DEFAULT_BUCKET}\"}\n",
    "\n",
    "data_folder = str(PROJECT_DIR / 'data')\n",
    "cfg, base_file = load_config_5_with_8_epochs(data_folder)\n",
    "\n",
    "hyperparameters = extract_hyperparams(entry_point) # custom function to parse the training script and extract config\n",
    "hyperparameters['base_file'] = base_file\n",
    "\n",
    "metrics = [\n",
    "    {\"Name\": \"train:loss_rpn_cls\", \"Regex\": \"loss_rpn_cls: ([0-9\\.]+)\"},\n",
    "    {\"Name\": \"train:loss_rpn_bbox\", \"Regex\": \"loss_rpn_bbox: ([0-9\\.]+)\"},\n",
    "    {\"Name\": \"train:loss_cls\", \"Regex\": \"loss_cls: ([0-9\\.]+)\"},\n",
    "    {\"Name\": \"train:loss_bbox\", \"Regex\": \"loss_bbox: ([0-9\\.]+)\"},\n",
    "    {\"Name\": \"train:loss\", \"Regex\": \"loss: ([0-9\\.]+)\"},\n",
    "    {\"Name\": \"train:accuracy\", \"Regex\": \"acc: ([0-9\\.]+)\"},\n",
    "    {\"Name\": \"train:epoch\", \"Regex\": \"Epoch (\\[[0-9\\.]+\\])\"},\n",
    "    {\"Name\": \"val:epoch\", \"Regex\": \"Epoch\\(val\\) (\\[[0-9]+\\])\"},\n",
    "    {\"Name\": \"val:mAP\", \"Regex\": \"mAP: ([0-9\\.]+)\"},\n",
    "]\n",
    "\n",
    "estimator = PyTorch(\n",
    "    entry_point=entry_point,             # This function will be called by the training job\n",
    "    source_dir=\"../src\",                 # All code in this folder will be copied over\n",
    "    image_uri=f\"954362353459.dkr.ecr.{DEFAULT_REGION}.amazonaws.com/sm-training-custom:torch-1.8.1-cu111-noGPL\",\n",
    "    role=role,\n",
    "    output_path=s3_output_location,\n",
    "    container_log_level=20,             # 10=debug, 20=info\n",
    "    base_job_name=exp_name,\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.g4dn.xlarge\",     # a GPU instance\n",
    "    volume_size=45,\n",
    "    metric_definitions=metrics,\n",
    "    hyperparameters=hyperparameters,\n",
    ")\n",
    "\n",
    "estimator.fit(\n",
    "    input_channels,\n",
    "    wait=False,           # Whether or not the notebook should wait for the job to finish. By setting it to False we can continue working while the job runs on another machine.\n",
    ")\n",
    "\n",
    "# save the name of the experiment to the filesystem so that we can use it later\n",
    "experiment_name = estimator._hyperparameters[\"sagemaker_job_name\"]\n",
    "\n",
    "with open(f'{PROJECT_DIR}/experiment_tut5_epoch_8.txt', 'w+') as f:\n",
    "    f.write(experiment_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/sagemaker-user/gdsc5-tutorials-public/experiment_tut5_epoch_8.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-af94d8e6da8b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# read the experiment name from the filesystem\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'{PROJECT_DIR}/experiment_tut5_epoch_8.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mexperiment_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmodel_location\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf'{s3_output_location}/{experiment_name}/output/model.tar.gz'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/sagemaker-user/gdsc5-tutorials-public/experiment_tut5_epoch_8.txt'"
     ]
    }
   ],
   "source": [
    "# read the experiment name from the filesystem\n",
    "with open(f'{PROJECT_DIR}/experiment_tut5_epoch_8.txt', 'r') as f:\n",
    "    experiment_name = f.read()\n",
    "\n",
    "model_location = f'{s3_output_location}/{experiment_name}/output/model.tar.gz'\n",
    "local_model_dir = download_and_extract_model(model_uri=model_location, local_dir='data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'experiment_name' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-028cf01a76d4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprediction_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'{data_folder}/{experiment_name}/results_tutorial5_test_epoch_8.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m';'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mrestricted_prediction_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprediction_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mprediction_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetection_score\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mrestricted_prediction_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'{data_folder}/results_tutorial5_epoch_8_min_detection_score.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m';'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'experiment_name' is not defined"
     ]
    }
   ],
   "source": [
    "prediction_df = pd.read_csv(f'{data_folder}/{experiment_name}/results_tutorial5_test_epoch_8.csv', sep=';')\n",
    "restricted_prediction_df = prediction_df[prediction_df.detection_score>0.5]\n",
    "restricted_prediction_df.to_csv(f'{data_folder}/results_tutorial5_epoch_8_min_detection_score.csv', sep=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the model for 8 epochs leads to a significantly higher leaderbord score of 110, instead of 90 for 4 epochs. So the model is clearly not overfitting!\\\n",
    "However, increasing model performance by training for more and more epochs is not an option. This is simply too time-consuming and too costly.\n",
    "\n",
    "Maybe our model just shows a low _convergence rate_ because it is not able to extract all relevant features from the input data?\\\n",
    "We will cover this in the following exercises.\n",
    "\n",
    "**Exercises:**\n",
    "- Think about possibilities to support our model in extracting features from input data. Hint: Check the size of the images we loaded in Tutorial 2, and compare them to the scale of the images in the model's configuration (look out for the parameter \"img_scale\").\n",
    "- Evaluate how the model performs on different stainings or studies. Should we train individual models for them?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This tutorial showed how we can systematically improve our model in an interative manner:\n",
    "- We learned how to apply CRISP-DM and the scientific method to plan, prioritize, and carry out iterations.\n",
    "- We formulated an initial list of hypotheses and adapted them on the basis of gained insights.\n",
    "- We made our model more powerful by increasing the maximal number of detectable sections.\n",
    "- We improved the model's predictions by making use of the provided detection scores as a measure of confidence.\n",
    "- We discussed properties of over- and underfitting and derived further hypotheses.\n",
    "\n",
    "Now you should be well-equipped to tackle the Global Data Science Challenge 2022.\\\n",
    "We wish you and your team good luck and a lot of fun!"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "python3 (gdsc5-smstudio-custom/1)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:314026059811:image-version/gdsc5-smstudio-custom/1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
