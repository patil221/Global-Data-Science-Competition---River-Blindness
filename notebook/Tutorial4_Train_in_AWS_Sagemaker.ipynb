{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 4: Train your model in an AWS Sagemaker training job\n",
    "\n",
    "In the last tutorial, we trained and evaluated a first model on small dummy data. While it is possible to train a model within [Sagemaker Studio](https://www.youtube.com/watch?v=uQc8Itd4UTs&list=PLhr1KZpdzukcOr_6j_zmSrvYnLUtgqsZz) it's better to use a Sagemaker training job instead. Sagemaker training jobs have several advantages over a normal notebook. Sagemaker training jobs, namely they\n",
    "\n",
    "- provide you with a nice overview of all the trainings you ran\n",
    "- automatically store the results of a training run (metrics, [logs](https://console.aws.amazon.com/cloudwatch) and models)\n",
    "- do not automatically shut down after a few hours (which we enabled in the notebooks)\n",
    "- allows to run multiple training jobs in parallel (if you have sufficient GPUs allocated)\n",
    "\n",
    "In this notebook, we will convert the approach from tutorial 3 into a Sagemaker training job and train our model on the full data set. During the training, the logs are send to [Cloudwatch](https://console.aws.amazon.com/cloudwatch) - aws' monitoring service. After the training completed, the model is saved and automatically uploaded to S3. From there we'll retrieve the model and evaluate it.\n",
    "\n",
    "**NOTE: We will NOT need a GPU for this tutorial notebook. Pick a non GPU instance type to save costs.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "As usual, we'll start by setting up the appropriate imports. Interacting with Sagemaker and starting a training job can be done in several ways (e.g. from the AWS UI). We'll use the [Sagemaker Python SDK](https://sagemaker.readthedocs.io/en/stable/overview.html#train-a-model-with-the-sagemaker-python-sdk), which is already installed in your studio environment, to start the training job from this notebook.\n",
    "\n",
    "*Hint: if you want to use this notebook on your local machine, make sure to set up the AWS CLI with your challenge credentials and install the requirements.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3           # For interacting with S3\n",
    "import pandas as pd\n",
    "import sys             # Python system library needed to load custom functions\n",
    "\n",
    "# Imports to run Sagemaker training jobs\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.pytorch import PyTorch\n",
    "from sagemaker.session import Session\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('../src')  # Add the source directory to the PYTHONPATH. This allows to import local functions and modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No CUDA runtime is found, using CUDA_HOME='/usr/local/cuda'\n"
     ]
    }
   ],
   "source": [
    "from config import DEFAULT_BUCKET, DEFAULT_REGION  # The name of the S3 bucket that contains the training data\n",
    "from detection_util import create_predictions\n",
    "from gdsc_util import download_and_extract_model, set_up_logging, extract_hyperparams,create_encrypted_bucket, PROJECT_DIR\n",
    "from tutorial_4_training import load_config\n",
    "\n",
    "set_up_logging()  # Sets up logging to console and .log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converting the Tutorial 3 into a Training Script\n",
    "\n",
    "Looking back at tutorial 3, you'll find that the notebook contained three main components that were necessary for training a model\n",
    "\n",
    "- the definition of the dataset\n",
    "- the configuration file of the training\n",
    "- the logic for building the dataset and starting the training\n",
    "\n",
    "How can we best turn this into a script? For starters, the definition of the dataset is not likely to change. Hence it makes sense to turn this into a separate module and import it where necessary. You can find the code for this in ```src/Dataset.py```. The configuration will probably be changed during different experiments. Similarly, the logic might change a bit once we start using different training and testing datasets. Therefore we put these two into a separate file called ```src/tutorial_4_training.py```. The intend is to have a separate file for each training job in order to make everything reproduceable and easily comparable. Note that for a longer project you'll probably want to set up a proper experiment tracking tool like [Sagemaker Experiments](https://sagemaker-experiments.readthedocs.io/en/latest/index.html) or [MLFlow](https://mlflow.org/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running the Training Script\n",
    "\n",
    "The training job will run on a virtual machine (called instance) in the AWS cloud. An overview of all your training jobs can be found in the [AWS console](https://us-east-1.console.aws.amazon.com/sagemaker/home?region=us-east-1#/jobs) (you may have to change the region) after you logged into your account. You can also navigate directly to *Amazon SageMaker > Training > Training jobs* and click on the name of the latest training job.\n",
    "\n",
    "To start, we need to set the name of our experiment. Keep in mind that every experiment should have a unique name. Since we'll use a separate python script for each training, we'll use the name of the training script as name of the experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tutorial-4-training'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entry_point = 'tutorial_4_training.py'\n",
    "exp_name = entry_point.split('.')[0].replace('_', '-')  # AWS does not allow . and _ as experiment names\n",
    "exp_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to define the AWS settings for the job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "account_id = boto3.client('sts').get_caller_identity().get('Account')\n",
    "role = get_execution_role()\n",
    "sm_client = boto3.client(\"sagemaker\", region_name=DEFAULT_REGION)\n",
    "sess = Session(sagemaker_client=sm_client)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also need to download the images and training csv in order to train a model. Luckily, Sagemaker has a built-in functionality for this. \n",
    "Via the ```input_channels``` parameter we can specify multiple S3 locations. The contents are downloaded in the training job and made available under the provided name (dictionary key).\n",
    "In the example below, Sagemaker will download the complete content of the training data bucket, store it on the instance and, save its location in an environment variable called ```SM_CHANNEL_TRAIN```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_channels = {    \n",
    "    \"train\": f\"s3://{DEFAULT_BUCKET}\"    \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also specify where Sagemaker should store the results of the training job. I.e. the weights of the trained model. You will also see the link at the bottom of the training job overview page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-07-04 12:07:09,366 - root - INFO - Successfully created encrypted bucket: sagemaker-us-east-1-314026059811\n"
     ]
    }
   ],
   "source": [
    "# we need to create our own s3 bucket if it doesn't exist yet:\n",
    "sagemaker_bucket = f\"sagemaker-{DEFAULT_REGION}-{account_id}\"\n",
    "create_encrypted_bucket(sagemaker_bucket)\n",
    "s3_output_location = f\"s3://{sagemaker_bucket}/{exp_name}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to track which configuration lead to which result, we want to attach our specified configuration to the training job. Therefore we must first load the configuration from the training script. Afterwards, we will extract the configuration which we explicitly specified and save it as a dictionary. Later on, we will pass this dictionary as hyperparameters to the training job.\n",
    "\n",
    "The hyperparameter dictionary will look like this: \n",
    "\n",
    "```json\n",
    "{\n",
    "    \"dataset_type\": \"'OnchoDatase'\",\n",
    "    \"data_root\": \"data_folder\",\n",
    "    \"data.train.type\": \"OnchoDataset\",\n",
    "    ...\n",
    "    \"evaluation.metric\": \"mAP\",\n",
    "    \"runner.max_epochs\": 4\n",
    "}\n",
    "```\n",
    "You fill find this configuration later in the training job overview in the AWS sagemaker UI.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = str(PROJECT_DIR / 'data')\n",
    "cfg, base_file = load_config(data_folder)\n",
    "hyperparameters = extract_hyperparams(entry_point) # custom function to parse the training script and extract config\n",
    "hyperparameters['base_file'] = base_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we need to specify which metrics we want Sagemaker to automatically track. For this we need to setup [regular expressions](https://en.wikipedia.org/wiki/Regular_expression) that will be applied on the logs.\n",
    "The corresponding values will then be stored and made visible in the training job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output format is:\n",
    "# INFO:mmdet:Epoch [4][50/497]#011lr: 5.000e-03, eta: 0:10:23, time: 1.638, data_time: 1.433, memory: 1863, loss_rpn_cls: 0.0897, loss_rpn_bbox: 0.0781, loss_cls: 0.2336, acc: 90.8691, loss_bbox: 0.3404, loss: 0.7418\n",
    "metrics = [\n",
    "    {\"Name\": \"train:loss_rpn_cls\", \"Regex\": \"loss_rpn_cls: ([0-9\\.]+)\"},\n",
    "    {\"Name\": \"train:loss_rpn_bbox\", \"Regex\": \"loss_rpn_bbox: ([0-9\\.]+)\"},\n",
    "    {\"Name\": \"train:loss_cls\", \"Regex\": \"loss_cls: ([0-9\\.]+)\"},\n",
    "    {\"Name\": \"train:loss_bbox\", \"Regex\": \"loss_bbox: ([0-9\\.]+)\"},\n",
    "    {\"Name\": \"train:loss\", \"Regex\": \"loss: ([0-9\\.]+)\"},\n",
    "    {\"Name\": \"train:accuracy\", \"Regex\": \"acc: ([0-9\\.]+)\"},\n",
    "    {\"Name\": \"train:epoch\", \"Regex\": \"Epoch (\\[[0-9\\.]+\\])\"},\n",
    "    {\"Name\": \"val:epoch\", \"Regex\": \"Epoch\\(val\\) (\\[[0-9]+\\])\"},\n",
    "    {\"Name\": \"val:mAP\", \"Regex\": \"mAP: ([0-9\\.]+)\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With all the preparations done, we can now create the *estimator object* that defines all the relevant settings, in particular the libraries that need to be installed. A few things to note:\n",
    "\n",
    "- Since our object detection library MMDetection is based on *Pytorch*, we will use the ```Pytorch``` estimator. With it, we are using a pytorch-based docker image which we extended for the challenge, such that we don't have to care about installing any of the preliminary libraries. Any additional requirement will be installed on the training instance after startup. If you need additional libraries you can just add them to the file ```src/requirements.txt```.\n",
    "- The training job will run our predefined entry point, i.e. the python script ```tutorial_4_training.py```. \n",
    "- When we start the training by calling ```estimator.fit()```, the whole content of the *src* directory will be uploaded to the training instance. This becomes relevant for importing other modules.\n",
    "- If you don't want your job to finish, you can stop it in the UI. Be advised that you can only run one job at a time during the challenge.\n",
    "- When your training job is completed, your model will be stored as a .tar file in S3. The ```s3_output_location``` determines the location. You will find it in the folder ```<your-training-job>/output```. You can download your model from there and test it locally. If there is no \"output\" folder, make sure to check the logs in the AWS console, your training job probably failed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = PyTorch(\n",
    "    entry_point=entry_point,             # This function will be called by the training job\n",
    "    source_dir=\"../src\",                 # All code in this folder will be copied over\n",
    "    image_uri=f\"954362353459.dkr.ecr.{DEFAULT_REGION}.amazonaws.com/sm-training-custom:torch-1.8.1-cu111-noGPL\",\n",
    "    role=role,\n",
    "    output_path=s3_output_location,\n",
    "    container_log_level=20,             # 10=debug, 20=info\n",
    "    base_job_name=exp_name,\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.g4dn.xlarge\",     # a GPU instance\n",
    "    volume_size=45,\n",
    "    metric_definitions=metrics,\n",
    "    hyperparameters=hyperparameters,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we created the estimator, we will need to call the .fit method to start the training job. As this might take a while, we set ```wait=False``` so our notebook will not wait for the training job to finish and we can continue working."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-06-06 18:22:25,252 - sagemaker.image_uris - INFO - Defaulting to the only supported framework/algorithm version: latest.\n",
      "2022-06-06 18:22:25,273 - sagemaker.image_uris - INFO - Ignoring unnecessary instance type: None.\n",
      "2022-06-06 18:22:25,576 - sagemaker - INFO - Creating training-job with name: tutorial-4-training-2022-06-06-18-22-25-249\n"
     ]
    }
   ],
   "source": [
    "estimator.fit(\n",
    "    input_channels,\n",
    "    wait=False,           # Whether or not the notebook should wait for the job to finish. By setting it to False we can continue working while the job runs on another machine.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model location to the filesystem so that we can use it later\n",
    "model_location = f'{s3_output_location}/{estimator._hyperparameters[\"sagemaker_job_name\"]}/output/model.tar.gz'\n",
    "print(model_location)\n",
    "\n",
    "with open(f'{PROJECT_DIR}/model_location.txt', 'w+') as f:\n",
    "    f.write(model_location)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One important question that we haven't answered to far is *How can I see how the training is doing?* After all you need see if there are any issues or if the training is going well.\n",
    "This can be done as follows:\n",
    "\n",
    "1. Go to the AWS Console\n",
    "2. Search for AWS Sagemaker and go to the Service\n",
    "3. On the left pane click on Training -> Training Jobs\n",
    "4. Your training job should the one at the very top. Click it.\n",
    "5. The details page has a link *View logs* in the *Monitor* section (scroll down). Click this to see the logs. \n",
    "\n",
    "The default configuration for this tutorial will train your model for 4 epochs. This should take around 3 hours and give you an indication if an idea is working or not.\n",
    "Training for longer does not necessary help. In our tests results rarely improved after ~12 epochs even with different configurations. **To save time and money we suggest to start with a few epochs and only train models for longer where you are certain that there is a significant benefit.** After all, you only have a limited budget."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A new submission with our newly trained model!\n",
    "\n",
    "After the training job is finished - which should take around 3 hours! - you can download the results, load the model and create a new submission!\n",
    "\n",
    "*Note: Check the AWS training job console to see the status of your training job*\n",
    "\n",
    "First we need to specify where the results where stored. We stored the model location to the local filesystem, we only need to read it. If that didn't work, make sure to check the Sagemaker Training section in the AWS console. The model location will look similar to this:```s3://sagemaker-us-east-1-954362353459/tutorial-4-training/tutorial-4-training-2022-06-06-18-22-25-249/output/model.tar.gz```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the model location from the filesystem\n",
    "with open(f'{PROJECT_DIR}/model_location.txt', 'r') as f:\n",
    "    model_location = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We provide a custom function that downloads the results to our local environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_model_dir = download_and_extract_model(model_uri=model_location, local_dir='data')\n",
    "local_model_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Navigate to the *data* directory and verify that everything went well. You should see the following files:\n",
    "\n",
    "- *epoch_1.pth*\n",
    "- *epoch_2.pth*\n",
    "- *epoch_3.pth*\n",
    "- *epoch_4.pth*\n",
    "- *model.tar.gz*\n",
    "- *None.log.json*\n",
    "\n",
    "The *.pth files contain the weights of the model. To use them we need to \n",
    "\n",
    "- load the corresponding config (we already did this above), \n",
    "- specify which checkpoint we want to use and,\n",
    "- load the names of the image files for which we want to create predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = f'{local_model_dir}/epoch_4.pth' # Select one of the model checkpoints to load in\n",
    "file_names = pd.read_csv(f'{data_folder}/test_files.csv', sep=';', header=None)[0].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this we are ready to create our second submission, similar to how we did it in tutorial 3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load checkpoint from local path: /root/data/AmazonSageMaker-gdsc-tutorials/data/tutorial-4-training-2022-05-27-15-05-00-955/epoch_4.pth\n",
      "2022-05-27 19:47:51,431 - detection_util - INFO - Creating predictions for 73 files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/73 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-05-27 19:47:51,437 - detection_util - INFO - Processing file: 100_D.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mmdetection/mmdet/datasets/utils.py:70: UserWarning: \"ImageToTensor\" pipeline is replaced by \"DefaultFormatBundle\" for batch inference. It is recommended to manually replace it in the test data pipeline in your config file.\n",
      "  'data pipeline in your config file.', UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022-05-27 19:47:52.945 gdsc5-smstudio-custom-ml-t3-medium-e966dd789eb1f61b988c87d4472b:1624 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\n",
      "[2022-05-27 19:47:53.009 gdsc5-smstudio-custom-ml-t3-medium-e966dd789eb1f61b988c87d4472b:1624 INFO profiler_config_parser.py:102] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|â–         | 1/73 [00:07<08:24,  7.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-05-27 19:47:58,445 - detection_util - INFO - Processing file: 100_C.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|â–Ž         | 2/73 [00:13<07:56,  6.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-05-27 19:48:04,950 - detection_util - INFO - Processing file: 100_B.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|â–         | 3/73 [00:18<06:41,  5.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-05-27 19:48:09,526 - detection_util - INFO - Processing file: 100_AA.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|â–Œ         | 4/73 [00:23<06:22,  5.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-05-27 19:48:14,779 - detection_util - INFO - Processing file: 100_A.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|â–‹         | 5/73 [00:29<06:22,  5.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-05-27 19:48:20,536 - detection_util - INFO - Processing file: 101_DD.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|â–Š         | 6/73 [00:34<06:11,  5.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-05-27 19:48:25,947 - detection_util - INFO - Processing file: 101_C.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|â–‰         | 7/73 [00:39<06:04,  5.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-05-27 19:48:31,433 - detection_util - INFO - Processing file: 101_B.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|â–ˆ         | 8/73 [00:45<05:57,  5.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-05-27 19:48:36,879 - detection_util - INFO - Processing file: 101_AA.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|â–ˆâ–        | 9/73 [00:50<05:49,  5.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-05-27 19:48:42,272 - detection_util - INFO - Processing file: 101_A.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|â–ˆâ–Ž        | 10/73 [00:55<05:34,  5.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-05-27 19:48:47,246 - detection_util - INFO - Processing file: 86_D.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|â–ˆâ–Œ        | 11/73 [01:03<06:11,  5.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-05-27 19:48:54,775 - detection_util - INFO - Processing file: 86_C.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|â–ˆâ–‹        | 12/73 [01:10<06:31,  6.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-05-27 19:49:02,175 - detection_util - INFO - Processing file: 86_B.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|â–ˆâ–Š        | 13/73 [01:18<06:45,  6.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-05-27 19:49:09,691 - detection_util - INFO - Processing file: 86_AA.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|â–ˆâ–‰        | 14/73 [01:24<06:24,  6.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-05-27 19:49:15,671 - detection_util - INFO - Processing file: 86_A.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|â–ˆâ–ˆ        | 15/73 [01:30<06:21,  6.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-05-27 19:49:22,362 - detection_util - INFO - Processing file: 88_D.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|â–ˆâ–ˆâ–       | 16/73 [01:38<06:23,  6.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-05-27 19:49:29,481 - detection_util - INFO - Processing file: 88_C.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|â–ˆâ–ˆâ–Ž       | 17/73 [01:46<06:43,  7.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-05-27 19:49:37,799 - detection_util - INFO - Processing file: 88_B.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|â–ˆâ–ˆâ–       | 18/73 [01:53<06:33,  7.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-05-27 19:49:44,818 - detection_util - INFO - Processing file: 88_A.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|â–ˆâ–ˆâ–Œ       | 19/73 [01:59<06:08,  6.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-05-27 19:49:50,895 - detection_util - INFO - Processing file: 89_D.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|â–ˆâ–ˆâ–‹       | 20/73 [02:04<05:27,  6.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-05-27 19:49:55,582 - detection_util - INFO - Processing file: 89_C.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|â–ˆâ–ˆâ–‰       | 21/73 [02:10<05:19,  6.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-05-27 19:50:01,615 - detection_util - INFO - Processing file: 89_B.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|â–ˆâ–ˆâ–ˆ       | 22/73 [02:15<04:53,  5.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-05-27 19:50:06,493 - detection_util - INFO - Processing file: 89_AA.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|â–ˆâ–ˆâ–ˆâ–      | 23/73 [02:21<04:55,  5.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-05-27 19:50:12,779 - detection_util - INFO - Processing file: 89_A.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 24/73 [02:26<04:39,  5.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-05-27 19:50:17,954 - detection_util - INFO - Processing file: 90_D.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|â–ˆâ–ˆâ–ˆâ–      | 25/73 [02:33<04:57,  6.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-05-27 19:50:25,305 - detection_util - INFO - Processing file: 90_C.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 26/73 [02:40<04:56,  6.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-05-27 19:50:31,857 - detection_util - INFO - Processing file: 90_B.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|â–ˆâ–ˆâ–ˆâ–‹      | 27/73 [02:47<05:03,  6.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-05-27 19:50:39,129 - detection_util - INFO - Processing file: 90_AA.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|â–ˆâ–ˆâ–ˆâ–Š      | 28/73 [02:54<04:58,  6.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-05-27 19:50:45,862 - detection_util - INFO - Processing file: 90_A.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|â–ˆâ–ˆâ–ˆâ–‰      | 29/73 [03:01<05:04,  6.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-05-27 19:50:53,417 - detection_util - INFO - Processing file: 91_D.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 30/73 [03:09<05:00,  6.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-05-27 19:51:00,589 - detection_util - INFO - Processing file: 91_C.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 31/73 [03:16<05:02,  7.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-05-27 19:51:08,326 - detection_util - INFO - Processing file: 91_B.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 32/73 [03:23<04:44,  6.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-05-27 19:51:14,591 - detection_util - INFO - Processing file: 91_AA.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 33/73 [03:29<04:29,  6.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-05-27 19:51:20,908 - detection_util - INFO - Processing file: 91_A.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 34/73 [03:35<04:17,  6.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-05-27 19:51:27,188 - detection_util - INFO - Processing file: 92_D.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 35/73 [03:42<04:07,  6.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-05-27 19:51:33,529 - detection_util - INFO - Processing file: 92_C.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 36/73 [03:48<03:57,  6.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-05-27 19:51:39,692 - detection_util - INFO - Processing file: 92_B.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 37/73 [03:56<04:08,  6.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-05-27 19:51:47,692 - detection_util - INFO - Processing file: 92_AA.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 38/73 [04:02<03:51,  6.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-05-27 19:51:53,678 - detection_util - INFO - Processing file: 92_A.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 39/73 [04:08<03:41,  6.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-05-27 19:51:59,960 - detection_util - INFO - Processing file: 93_D.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 40/73 [04:16<03:48,  6.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-05-27 19:52:07,863 - detection_util - INFO - Processing file: 93_C.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 41/73 [04:24<03:56,  7.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-05-27 19:52:16,334 - detection_util - INFO - Processing file: 93_B.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 42/73 [04:33<03:57,  7.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-05-27 19:52:24,576 - detection_util - INFO - Processing file: 93_AA.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 43/73 [04:39<03:36,  7.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-05-27 19:52:30,741 - detection_util - INFO - Processing file: 93_A.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 44/73 [04:44<03:13,  6.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-05-27 19:52:36,211 - detection_util - INFO - Processing file: 94_D.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 45/73 [04:51<03:09,  6.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-05-27 19:52:43,220 - detection_util - INFO - Processing file: 94_C.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 46/73 [04:57<02:54,  6.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-05-27 19:52:48,909 - detection_util - INFO - Processing file: 94_B.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 47/73 [05:05<03:03,  7.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-05-27 19:52:57,324 - detection_util - INFO - Processing file: 94_AA.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 48/73 [05:11<02:48,  6.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-05-27 19:53:03,316 - detection_util - INFO - Processing file: 94_A.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 49/73 [05:19<02:47,  6.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-05-27 19:53:10,846 - detection_util - INFO - Processing file: 95_D.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 50/73 [05:26<02:40,  6.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-05-27 19:53:17,904 - detection_util - INFO - Processing file: 95_C.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 51/73 [05:32<02:28,  6.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-05-27 19:53:24,036 - detection_util - INFO - Processing file: 95_B.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 52/73 [05:39<02:23,  6.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-05-27 19:53:31,128 - detection_util - INFO - Processing file: 95_AA.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 53/73 [05:46<02:18,  6.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-05-27 19:53:38,294 - detection_util - INFO - Processing file: 95_A.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 54/73 [05:53<02:07,  6.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-05-27 19:53:44,531 - detection_util - INFO - Processing file: 96_D.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 55/73 [05:59<01:57,  6.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-05-27 19:53:50,648 - detection_util - INFO - Processing file: 96_C.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 56/73 [06:04<01:44,  6.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-05-27 19:53:55,783 - detection_util - INFO - Processing file: 96_B.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 57/73 [06:10<01:37,  6.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-05-27 19:54:01,810 - detection_util - INFO - Processing file: 96_AA.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 58/73 [06:17<01:36,  6.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-05-27 19:54:08,926 - detection_util - INFO - Processing file: 96_A.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 59/73 [06:23<01:28,  6.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-05-27 19:54:15,096 - detection_util - INFO - Processing file: 97_D.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 60/73 [06:30<01:23,  6.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-05-27 19:54:21,679 - detection_util - INFO - Processing file: 97_C.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 61/73 [06:36<01:18,  6.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-05-27 19:54:28,397 - detection_util - INFO - Processing file: 97_B.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 62/73 [06:43<01:12,  6.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-05-27 19:54:35,235 - detection_util - INFO - Processing file: 97_A.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 63/73 [06:50<01:06,  6.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-05-27 19:54:41,984 - detection_util - INFO - Processing file: 98_D.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 64/73 [06:57<01:00,  6.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-05-27 19:54:49,051 - detection_util - INFO - Processing file: 98_C.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 65/73 [07:03<00:52,  6.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-05-27 19:54:55,152 - detection_util - INFO - Processing file: 98_B.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 66/73 [07:10<00:45,  6.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-05-27 19:55:01,499 - detection_util - INFO - Processing file: 98_AA.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 67/73 [07:16<00:39,  6.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-05-27 19:55:08,423 - detection_util - INFO - Processing file: 98_A.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 68/73 [07:23<00:33,  6.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-05-27 19:55:15,234 - detection_util - INFO - Processing file: 99_D.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 69/73 [07:30<00:26,  6.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-05-27 19:55:21,880 - detection_util - INFO - Processing file: 99_C.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 70/73 [07:35<00:18,  6.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-05-27 19:55:27,269 - detection_util - INFO - Processing file: 99_B.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 71/73 [07:41<00:12,  6.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-05-27 19:55:33,335 - detection_util - INFO - Processing file: 99_AA.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 72/73 [07:47<00:05,  5.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-05-27 19:55:38,612 - detection_util - INFO - Processing file: 99_A.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 73/73 [07:53<00:00,  6.49s/it]\n"
     ]
    }
   ],
   "source": [
    "prediction_df = create_predictions(file_names, cfg, checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how the creation of the predictions takes a lot longer than in the last tutorial. This is due to to the instance type. In tutorial 3, we were using an instance with a GPU. In this tutorial, we are not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_df.to_csv(f'{data_folder}/results_tutorial4_epoch_4.csv', sep=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After uploading them to the [GDSC website](https://gdsc.ce.capgemini.com/) you should get a score of around 75.\n",
    "\n",
    "**Exercise:**\n",
    "- Load the weights from a different checkpoint and use them to create a new submission. Do the results improve?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this tutorial, we took our knowledge from the last tutorial and used it to train a model in the cloud! â˜ï¸â˜ï¸  <br>\n",
    "Awesome, right? Let's quickly recap what we did. We\n",
    "\n",
    "* converted tutorial 3 into a training job in the cloud, using the Sagemaker SDK.\n",
    "* trained a model on the full data!\n",
    "* downloaded the trained model and created a second submission.\n",
    "\n",
    "And that's it for this tutorial already. Nice job ðŸš€ Next up: Tutorial 5 - Where to go from here"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "interpreter": {
   "hash": "0e1dd59bcc72e4774a40936d3cead7a89f28ea3d0929c42b28369e7650240f70"
  },
  "kernelspec": {
   "display_name": "python3 (gdsc5-smstudio-custom/1)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:314026059811:image-version/gdsc5-smstudio-custom/1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
