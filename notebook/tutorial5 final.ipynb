{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3           # For interacting with S3\n",
    "import pandas as pd\n",
    "import sys             # Python system library needed to load custom functions\n",
    "\n",
    "# Imports to run Sagemaker training jobs\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.pytorch import PyTorch\n",
    "from sagemaker.session import Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('../src')  # Add the source directory to the PYTHONPATH. This allows to import local functions and modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No CUDA runtime is found, using CUDA_HOME='/usr/local/cuda'\n"
     ]
    }
   ],
   "source": [
    "from config import DEFAULT_BUCKET, DEFAULT_REGION  # The name of the S3 bucket that contains the training data\n",
    "from detection_util import create_predictions\n",
    "from gdsc_util import download_and_extract_model, set_up_logging, extract_hyperparams, PROJECT_DIR\n",
    "from tutorial_4_training import load_config as load_config_4\n",
    "from tutorial_5_training_4k import load_config as load_config_5\n",
    "from tutorial_5_training_10_epochs import load_config as load_config_5_with_10_epochs\n",
    "from gdsc_util import load_sections_df\n",
    "from PredictionEvaluator import PredictionEvaluator\n",
    "from gdsc_score import get_leaderboard_score\n",
    "\n",
    "set_up_logging()  # Sets up logging to console and .log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from imutils import contours\n",
    "from skimage import measure\n",
    "import numpy as np\n",
    "#import imutils\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm\n",
    "from os import listdir\n",
    "from os.path import isfile, join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'cv2' has no attribute 'tutorial_5_training_4k'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-422a42ea132a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# OpenCv window to display the image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnamedWindow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'image'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtutorial_5_training_4k\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresizeWindow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'image'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m960\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m645\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'cv2' has no attribute 'tutorial_5_training_4k'"
     ]
    }
   ],
   "source": [
    "# OpenCv window to display the image\n",
    "cv2.namedWindow('image', cv2.tutorial_5_training_4k)\n",
    "cv2.resizeWindow('image', 960, 645)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/PIL/Image.py:2921: DecompressionBombWarning: Image size (147949802 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  DecompressionBombWarning,\n",
      "/opt/conda/lib/python3.6/site-packages/PIL/Image.py:2921: DecompressionBombWarning: Image size (146216239 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  DecompressionBombWarning,\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import exif\n",
    "import PIL\n",
    "from PIL import Image\n",
    " \n",
    "img_paths = glob.glob('../data/jpgs/*.jpg')\n",
    " \n",
    "for img_path in img_paths:\n",
    "    img = PIL.Image.open(img_path)\n",
    "    if not img.getexif(): # No EXIF tag at all\n",
    "        continue \n",
    "  \n",
    "    # Load Image EXIF\n",
    "    with open(img_path, 'rb') as f:\n",
    "        img_exif = exif.Image(f)\n",
    " # Delete orientation tag and store the image \n",
    " \n",
    "    if 'orientation' in dir(img_exif):\n",
    "        print(img_path)\n",
    "        img_exif.delete('orientation')\n",
    "        with open(img_path, 'wb') as f:\n",
    "            f.write(img_exif.get_file())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://jpg-team03/test_files.csv'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Uploading fixed images to YOUR S3 bucket\n",
    "#Uploading fixed images to YOUR S3 bucket\n",
    "import os\n",
    "from gdsc_util import upload_to_s3, PROJECT_DIR\n",
    "your_bucket_name = 'jpg-team03'\n",
    "files = os.listdir(f'{PROJECT_DIR}/data/jpgs')\n",
    "files = [i for i in files if i.endswith('jpg')]\n",
    "for file in files:\n",
    "    upload_to_s3(f'{PROJECT_DIR}/data/jpgs/{file}', f'jpgs/{file}', f'{your_bucket_name}')\n",
    "    \n",
    "    \n",
    "upload_to_s3(f'{PROJECT_DIR}/data/gdsc_train.csv', 'gdsc_train.csv', f'{your_bucket_name}')\n",
    "upload_to_s3(f'{PROJECT_DIR}/data/test_files.csv', 'test_files.csv', f'{your_bucket_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 'epoch_10'  # Select one of the model checkpoints to load in    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "entry_point = 'tutorial_5_training_4k.py'\n",
    "exp_name = entry_point.split('.')[0].replace('_', '-')  # AWS does not allow . and _ as experiment names\n",
    "account_id = boto3.client('sts').get_caller_identity().get('Account')\n",
    "role = get_execution_role()\n",
    "sm_client = boto3.client(\"sagemaker\", region_name=DEFAULT_REGION)\n",
    "sess = Session(sagemaker_client=sm_client)\n",
    "s3_output_location = f\"s3://{sess.default_bucket()}/{exp_name}\"\n",
    "input_channels = {\"train\": f\"s3://{your_bucket_name}\"}\n",
    "hyperparameters = extract_hyperparams(entry_point) # custom function to parse the training script and extract config\n",
    "hyperparameters['base_file'] = base_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = [\n",
    "    {\"Name\": \"train:loss_rpn_cls\", \"Regex\": \"loss_rpn_cls: ([0-9\\.]+)\"},\n",
    "    {\"Name\": \"train:loss_rpn_bbox\", \"Regex\": \"loss_rpn_bbox: ([0-9\\.]+)\"},\n",
    "    {\"Name\": \"train:loss_cls\", \"Regex\": \"loss_cls: ([0-9\\.]+)\"},\n",
    "    {\"Name\": \"train:loss_bbox\", \"Regex\": \"loss_bbox: ([0-9\\.]+)\"},\n",
    "    {\"Name\": \"train:loss\", \"Regex\": \"loss: ([0-9\\.]+)\"},\n",
    "    {\"Name\": \"train:accuracy\", \"Regex\": \"acc: ([0-9\\.]+)\"},\n",
    "    {\"Name\": \"train:epoch\", \"Regex\": \"Epoch (\\[[0-9\\.]+\\])\"},\n",
    "    {\"Name\": \"val:epoch\", \"Regex\": \"Epoch\\(val\\) (\\[[0-9]+\\])\"},\n",
    "    {\"Name\": \"val:mAP\", \"Regex\": \"mAP: ([0-9\\.]+)\"},\n",
    "]\n",
    "\n",
    "estimator = PyTorch(\n",
    "    entry_point=entry_point,             # This function will be called by the training job\n",
    "    source_dir=\"../src\",                 # All code in this folder will be copied over\n",
    "    image_uri=f\"954362353459.dkr.ecr.{DEFAULT_REGION}.amazonaws.com/sm-training-custom:torch-1.8.1-cu111-noGPL\",\n",
    "    role=role,\n",
    "    output_path=s3_output_location,\n",
    "    container_log_level=20,             # 10=debug, 20=info\n",
    "    base_job_name=exp_name,\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.g4dn.xlarge\",     # a GPU instance\n",
    "    volume_size=65,\n",
    "    metric_definitions=metrics,\n",
    "    hyperparameters=hyperparameters,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-07-28 05:42:54,436 - sagemaker.image_uris - INFO - Defaulting to the only supported framework/algorithm version: latest.\n",
      "2022-07-28 05:42:54,453 - sagemaker.image_uris - INFO - Ignoring unnecessary instance type: None.\n",
      "2022-07-28 05:42:54,733 - sagemaker - INFO - Creating training-job with name: tutorial-5-training-4k-2022-07-28-05-42-54-434\n"
     ]
    }
   ],
   "source": [
    "estimator.fit(\n",
    "    input_channels,\n",
    "    wait=False,           # Whether or not the notebook should wait for the job to finish. By setting it to False we can continue working while the job runs on another machine.\n",
    ")\n",
    "\n",
    "# save the name of the experiment to the filesystem so that we can use it later\n",
    "experiment_name = estimator._hyperparameters[\"sagemaker_job_name\"]\n",
    "\n",
    "with open(f'{PROJECT_DIR}/experiment_tut5_max_per_img.txt', 'w+') as f:\n",
    "    f.write(experiment_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-07-28 11:07:19,946 - gdsc_util - INFO - File tutorial-5-training-4k/tutorial-5-training-4k-2022-07-28-05-42-54-434/output/model.tar.gz already exists. Skipping download\n"
     ]
    }
   ],
   "source": [
    "# read the experiment name from the filesystem\n",
    "with open(f'{PROJECT_DIR}/experiment_tut5_max_per_img.txt', 'r') as f:\n",
    "    experiment_name = f.read()\n",
    "    \n",
    "experiment_name = 'tutorial-5-training-4k-2022-07-28-05-42-54-434'\n",
    "model_location = f'{s3_output_location}/{experiment_name}/output/model.tar.gz'\n",
    "local_model_dir = download_and_extract_model(model_uri=model_location, local_dir='data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name = 'tutorial-5-training-4k-2022-07-28-05-42-54-434'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_df = pd.read_csv(f'{data_folder}/{experiment_name}/results_tutorial5_test_epoch_8.csv', sep=';')\n",
    "restricted_prediction_df = prediction_df[prediction_df.detection_score>0.5]\n",
    "restricted_prediction_df.to_csv(f'{data_folder}/results_tutorial5_epoch_10_min_detection_score.csv', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageFilter\n",
    "image = Image.open(f'{PROJECT_DIR}/data/jpgs/102_A.jpg')\n",
    "  \n",
    "# Converting the image to grayscale, as edge detection \n",
    "# requires input image to be of mode = Grayscale (L)\n",
    "image = image.convert(\"L\")\n",
    "  \n",
    "# Detecting Edges on the Image using the argument ImageFilter.FIND_EDGES\n",
    "image = image.filter(ImageFilter.FIND_EDGES)\n",
    "  \n",
    "# Saving the Image Under the name Edge_Sample.png\n",
    "#image.save(f'{PROJECT_DIR}/data/jpgs/Edge_Sample.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "python3 (gdsc5-smstudio-custom/1)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:314026059811:image-version/gdsc5-smstudio-custom/1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
